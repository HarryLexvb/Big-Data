{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sH6gzPq9CZox",
        "outputId": "a5772935-0edb-4af7-ba76-868aca01c9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA no está disponible. Usando CPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA está disponible. Usando GPU.\")\n",
        "else:\n",
        "    print(\"CUDA no está disponible. Usando CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYwq__YVF-zQ",
        "outputId": "9c96ebd7-7cc9-4620-b539-66ba624237b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba) (0.43.0)\n",
            "Requirement already satisfied: numpy<2.1,>=1.22 in /usr/local/lib/python3.10/dist-packages (from numba) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTm0OfuyHo3N",
        "outputId": "82bae17c-eb91-41b4-bd23-990c7859eb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# navegar en google drive\n",
        "%cd /content/drive/MyDrive/big data/\n",
        "# mostrar el contenido\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksSAMUTKILua",
        "outputId": "6f8a287d-2675-43a3-e6d7-89b3066d2cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/big data\n",
            "large_text_file.txt  word_counter.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "\n",
        "# Descargar el corpus de palabras de nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l21yETbFHQou",
        "outputId": "22c18f31-12c2-4767-83b3-6b49dbd82f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import random\n",
        "from numba import cuda\n",
        "import numpy as np\n",
        "\n",
        "# corpus de palabras de nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "\n",
        "# palabras en inglés de nltk\n",
        "word_list = words.words()\n",
        "\n",
        "# Filtrar palabras que solo contienen letras\n",
        "word_list = [word for word in word_list if word.isalpha()]\n",
        "\n",
        "@cuda.jit\n",
        "def generate_words_gpu(word_indices, block_size, output):\n",
        "    pos = cuda.grid(1)\n",
        "    if pos < len(output):\n",
        "        word_idx = word_indices[pos % block_size]\n",
        "        output[pos] = word_idx\n",
        "\n",
        "def generate_large_text_file(filename, word_list, target_size_gb=10):\n",
        "\n",
        "    target_size_bytes = target_size_gb * (1024 ** 3)\n",
        "\n",
        "    block_size = 1000\n",
        "    word_indices = np.array([random.randint(0, len(word_list) - 1) for _ in range(block_size)], dtype=np.int32)\n",
        "    word_indices_gpu = cuda.to_device(word_indices)\n",
        "\n",
        "    word_len_avg = sum(len(word) for word in word_list) / len(word_list)\n",
        "    approx_num_words = target_size_bytes // int(word_len_avg + 1)\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        words_per_write = 1000000\n",
        "        output_gpu = cuda.device_array(words_per_write, dtype=np.int32)\n",
        "\n",
        "        for _ in range(approx_num_words // words_per_write):\n",
        "            generate_words_gpu[(words_per_write + block_size - 1) // block_size, block_size](word_indices_gpu, block_size, output_gpu)\n",
        "            output_host = output_gpu.copy_to_host()\n",
        "            words_to_write = ' '.join(word_list[idx] for idx in output_host)\n",
        "            f.write(words_to_write + ' ')\n",
        "\n",
        "    print(f\"Archivo {filename} generado exitosamente.\")\n",
        "\n",
        "# Ruta en Google Drive donde se guardará el archivo\n",
        "file_path = '/content/drive/MyDrive/big data/large_text_file.txt'\n",
        "\n",
        "# Generar archivo de 10 GB en Google Drive\n",
        "generate_large_text_file(file_path, word_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW9AznluIl3C",
        "outputId": "c6b90c66-da20-49b0-c672-56a96492e621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo /content/drive/MyDrive/big data/large_text_file.txt generado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import Counter\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import re\n"
      ],
      "metadata": {
        "id": "ONY6d3wEh5x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(chunk):\n",
        "    words = re.findall(r'\\b\\w+\\b', chunk.decode('utf-8').lower())\n",
        "    return len(words), Counter(words)\n",
        "\n",
        "def word_count(file_path, chunk_size=64 * 1024 * 1024):\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    total_words = 0\n",
        "    combined_counter = Counter()\n",
        "\n",
        "    with open(file_path, 'rb') as f, ProcessPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for _ in range(0, file_size, chunk_size):\n",
        "            chunk = f.read(chunk_size)\n",
        "            futures.append(executor.submit(process_chunk, chunk))\n",
        "\n",
        "        for future in futures:\n",
        "            word_count, word_counter = future.result()\n",
        "            total_words += word_count\n",
        "            combined_counter.update(word_counter)\n",
        "\n",
        "    return total_words, combined_counter\n",
        "\n",
        "file_path = '/content/drive/MyDrive/big data/large_text_file.txt'\n",
        "\n",
        "total_words, word_counter = word_count(file_path)\n",
        "\n",
        "with open('/content/drive/MyDrive/big data/word_count_result.txt', 'w') as result_file:\n",
        "    result_file.write(f'Total words: {total_words}\\n')\n",
        "    result_file.write('Top 10 most common words:\\n')\n",
        "    for word, count in word_counter.most_common(10):\n",
        "        result_file.write(f'{word}: {count}\\n')\n",
        "\n",
        "print(f'Total words: {total_words}')\n",
        "print(f'Los resultados han sido guardados en /content/drive/MyDrive/big data/word_count_result.txt')\n"
      ],
      "metadata": {
        "id": "oZEaWtF5oHoh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19cda5d-d818-40ec-cb3b-299425306b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 1073000133\n",
            "Los resultados han sido guardados en /content/drive/MyDrive/big data/word_count_result.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[github](https://github.com/HarryLexvb/Big-Data)"
      ],
      "metadata": {
        "id": "XW3yQ4WCOSMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "github: https://github.com/HarryLexvb/Big-Data"
      ],
      "metadata": {
        "id": "-Ye6f-0-OJCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}